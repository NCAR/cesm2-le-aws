{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd9a6f5-cad8-4601-8d17-d554143ae036",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import shutil\n",
    "from functools import reduce, partial\n",
    "from operator import mul\n",
    "import yaml\n",
    "\n",
    "import xarray as xr\n",
    "import yaml\n",
    "from distributed import Client\n",
    "from distributed.utils import format_bytes\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import dask\n",
    "import intake\n",
    "from ncar_jobqueue import NCARCluster\n",
    "from helpers import (create_grid_dataset, enforce_chunking, get_grid_vars,\n",
    "                     print_ds_info, process_variables, save_data, zarr_store, fix_time, inspect_written_stores)\n",
    "\n",
    "#dask.config.set({\"distributed.dashboard.link\": \"/proxy/{port}/status\"})\n",
    "xr.set_options(keep_attrs=True)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54597cb1-a07a-48aa-9f3c-02f6041cb466",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = NCARCluster(memory=\"10GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcae9552-a059-41ce-9334-dd4f63cbf16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b07561-7d37-47c9-8781-3590006f48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adecb75-e94a-487e-ab4e-252358b9e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c960e-3250-4894-87a0-658c845fa1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = intake.open_esm_datastore(\n",
    "    \"/glade/work/mgrover/intake-esm-catalogs/new-cesm2-le.json\",\n",
    ")\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d5256-e760-49ef-8fb2-7aa9934d7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d12fd-dc73-434a-93dd-4bd1bac23752",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirout = \"/glade/scratch/abanihi/lens2-aws\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02967a49-5326-4cfe-8c6c-d24c1f357288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(ds, variables):\n",
    "    \"\"\"Drop all unnecessary variables and coordinates\"\"\"\n",
    "\n",
    "    vars_to_drop = [vname for vname in ds.data_vars if vname not in variables]\n",
    "    coord_vars = [\n",
    "        vname\n",
    "        for vname in ds.data_vars\n",
    "        if \"time\" not in ds[vname].dims or \"bound\" in vname or \"bnds\" in vname\n",
    "    ]\n",
    "    ds_fixed = ds.set_coords(coord_vars)\n",
    "    data_vars_dims = []\n",
    "    for data_var in ds_fixed.data_vars:\n",
    "        data_vars_dims.extend(list(ds_fixed[data_var].dims))\n",
    "    coords_to_drop = [\n",
    "        coord for coord in ds_fixed.coords if coord not in data_vars_dims\n",
    "    ]\n",
    "    grid_vars = list(\n",
    "        set(vars_to_drop + coords_to_drop)\n",
    "        - set([\"time\", \"time_bound\", \"time_bnds\", \"time_bounds\"])\n",
    "    )\n",
    "    ds_fixed = ds_fixed.drop(grid_vars).reset_coords()\n",
    "    if \"history\" in ds_fixed.attrs:\n",
    "        del ds_fixed.attrs[\"history\"]\n",
    "    return ds_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb728f4f-406b-49bd-8d35-db8cdf05b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables=['T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8739c158-072d-4fb3-9269-32329a9b7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaabfb4-3159-4353-995a-b673cc2d9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = []\n",
    "variables = []\n",
    "for component, stream_val in config.items():\n",
    "    for stream, v in stream_val.items():\n",
    "        frequency = v[\"frequency\"]\n",
    "        freq = v[\"freq\"]\n",
    "        time_bounds_dim = v[\"time_bounds_dim\"]\n",
    "        variable_categories = list(v[\"variable_category\"].keys())\n",
    "        \n",
    "        for v_cat in variable_categories:\n",
    "            experiments = list(\n",
    "                v[\"variable_category\"][v_cat][\"experiment\"].keys()\n",
    "            )\n",
    "            for exp in experiments:\n",
    "                chunks = v[\"variable_category\"][v_cat][\"experiment\"][exp][\n",
    "                        \"chunks\"\n",
    "                    ]\n",
    "                variable = v[\"variable_category\"][v_cat][\"variable\"]\n",
    "                variables.extend(variable)\n",
    "                \n",
    "                col_subset, query = process_variables(\n",
    "                        col, variable, component, stream, exp\n",
    "                    )\n",
    "                \n",
    "                if not col_subset.df.empty:\n",
    "                        d = {\n",
    "                            \"query\": query,\n",
    "                            \"col\": col_subset,\n",
    "                            \"chunks\": chunks,\n",
    "                            \"frequency\": frequency,\n",
    "                            \"freq\": freq,\n",
    "                            \"time_bounds_dim\": time_bounds_dim,\n",
    "                        }\n",
    "                run_config.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3474a-3d08-4251-b6f4-d160cb52095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42a47f-971e-452c-9cbb-e76648ecd2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_chunk_size(ds):\n",
    "    ntime = len(ds.time)       # the number of time slices\n",
    "    chunksize_optimal = 100e6  # desired chunk size in bytes\n",
    "    ncfile_size = ds.nbytes    # the netcdf file size\n",
    "    chunksize = max(int(ntime* chunksize_optimal/ ncfile_size),1)\n",
    "\n",
    "    target_chunks = ds.dims.mapping\n",
    "    target_chunks['time'] = chunksize \n",
    "    \n",
    "    return target_chunks # a dictionary giving the chunk sizes in each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e509e9-7444-4e32-9ab3-2233f6fb5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_separator = '.'\n",
    "for run in tqdm(run_config, desc=\"runs\"):\n",
    "    print(\"*\" * 120)\n",
    "    query = run[\"query\"]\n",
    "    print(f\"query = {query}\")\n",
    "    frequency = run[\"frequency\"]\n",
    "    chunks = run[\"chunks\"]\n",
    "    cftime_freq = run[\"freq\"]\n",
    "    time_bounds_dim = run[\"time_bounds_dim\"]\n",
    "    \n",
    "    #if query[\"experiment\"] == \"20C\" and query[\"stream\"] == \"cice.h1\":\n",
    "    #    if query[\"component\"] == \"ice_sh\":\n",
    "    #        preprocess = _preprocess_ice_sh\n",
    "    #    elif query[\"component\"] == \"ice_nh\":\n",
    "    #        preprocess = _preprocess_ice_nh\n",
    "    #elif query[\"component\"] == \"lnd\":\n",
    "    #    preprocess = _preprocess_lnd\n",
    "    #elif query[\"component\"] == \"atm\":\n",
    "    #    preprocess = _preprocess_atm\n",
    "        \n",
    "    #print(preprocess.__name__)\n",
    "    \n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        dsets = run[\"col\"].to_dataset_dict(\n",
    "            cdf_kwargs={\"chunks\": chunks, \"decode_times\": True, \"use_cftime\": True},\n",
    "            progressbar=True,\n",
    "        )\n",
    "    \n",
    "    dsets = enforce_chunking(dsets, chunks, field_separator)\n",
    "    \n",
    "    for key, ds in tqdm(dsets.items(), desc=\"Saving zarr store\"):\n",
    "        ds = ds.sortby('time')\n",
    "        ds = _preprocess(ds, query['variable'])\n",
    "        chunks = determine_chunk_size(ds)\n",
    "        print(ds.get_index(\"time\").is_monotonic_increasing)\n",
    "        key = key.split(field_separator)\n",
    "        component = query['component']\n",
    "        experiment = query['experiment']\n",
    "        stream = query['stream']\n",
    "        forcing_variant = key[-2]\n",
    "        variable = key[-1]\n",
    "        \n",
    "        if frequency != \"hourly6\":\n",
    "        \n",
    "            if experiment == 'historical':\n",
    "            \n",
    "                start_time = \"1850-01\"\n",
    "                end_time = \"2015-01\"\n",
    "                ds = fix_time(\n",
    "                            ds,\n",
    "                            start=start_time,\n",
    "                            end=end_time,\n",
    "                            freq=cftime_freq,\n",
    "                            time_bounds_dim=time_bounds_dim,\n",
    "                        )\n",
    "            \n",
    "                store = zarr_store(experiment,\n",
    "                                   component,\n",
    "                                   frequency, \n",
    "                                   forcing_variant,\n",
    "                                   variable,\n",
    "                                   write=False,\n",
    "                                   dirout=dirout\n",
    "                                  )\n",
    "            \n",
    "                save_data(ds, store)\n",
    "                \n",
    "            elif experiment == 'ssp370':\n",
    "                start_time = \"2015-01\"\n",
    "                end_time = \"2101-01\"\n",
    "                ds = fix_time(\n",
    "                            ds,\n",
    "                            start=start_time,\n",
    "                            end=end_time,\n",
    "                            freq=cftime_freq,\n",
    "                            time_bounds_dim=time_bounds_dim,\n",
    "                        )\n",
    "            \n",
    "                store = zarr_store(experiment,\n",
    "                                   component,\n",
    "                                   frequency, \n",
    "                                   forcing_variant,\n",
    "                                   variable,\n",
    "                                   write=False,\n",
    "                                   dirout=dirout\n",
    "                                  )\n",
    "            \n",
    "                save_data(ds, store)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-cesm2-marbl]",
   "language": "python",
   "name": "conda-env-miniconda3-cesm2-marbl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
